We will use the chain rule. First, let us denote:
$$f_1 = xW_1 + b_1$$
$$f_2 = hW_2 + b_2$$
By the chain rule we get:
$$\frac{\partial CE}{\partial x} = \frac{\partial CE}{\partial softmax} * \frac{\partial softmax}{\partial f_2} * \frac{\partial f_2}{\partial h} * \frac{\partial h}{\partial \sigma} * \frac{\partial \sigma}{\partial f_1}* \frac{\partial f_1}{\partial x}$$
Now let us calculate each of the partial derivatives \& then we can substitue.
As we've seen in the previous question:
$$
\frac{\partial CE}{\partial softmax} * \frac{\partial softmax}{\partial f_2}=
\begin{cases}
\hat{y}_i - 1, \text{ } i=t  \\ 
\hat{y}_i, \text{     } i\neq t
\end{cases} = \hat{y}-y=softmax(f_2)-y
$$
where $\hat{y}=softmax(f_2)$ and $t$ is the index in which $y$ has the 1.\\
$$\frac{\partial f_2}{\partial h} = W_2^T$$
$$\frac{\partial h}{\partial \sigma} = \sigma(1-\sigma)$$
$$\frac{\partial f_1}{\partial x} = W_1^T$$
We end up with:
$$(softmax(f_2)-y) * W_2^T * (\sigma(f_1)(1-\sigma(f_1))) * W_1^T$$
Note that we need to turn $(\sigma(f_1)(1-\sigma(f_1)))$ into a square and diagonal matrix with size $D_h^2$.