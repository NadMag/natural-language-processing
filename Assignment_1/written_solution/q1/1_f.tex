The word2vec method is based around the distributional hypothesis: 
linguistic items with similar distributions have similar meanings.
Following the derivation in \cite{Levy2014Factorization}, 
we see that the word2vec algorithm implicitly factorizes a shifted Pointwise Mutual Information matrix of the respective word and context pairs.
Applied to NLP, the PMI of two words measures how much more they co-occur, than we would have a priori expected them to appear by chance.